{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as sk_preprocessing\n",
    "from sklearn import linear_model as sk_linear_model\n",
    "from sklearn import metrics as sk_metrics\n",
    "from matplotlib.pyplot import subplot, scatter, plot, axis, grid, figure, subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code block generates a test dataset based upon a ground truth function and some artifical noise. \n",
    "### Parameters: \n",
    "# number of datapoints \n",
    "n = 20; \n",
    "# minimum and maximum x value \n",
    "x_min = -10; \n",
    "x_max = 10; \n",
    "# standard deviation of the gaussian noise added\n",
    "sigma = 25; \n",
    "# underlying ground truth function\n",
    "def myFunc(x):\n",
    "    y = 3*x*x*x - x + 5\n",
    "    return y\n",
    "\n",
    "### Code: \n",
    "# sample the random points for the dataset between x_min and x_max\n",
    "x_sample = np.random.rand(n, 1)*(x_max - x_min) + x_min\n",
    "# evaluate the ground truth function and add noise to the result \n",
    "y_sample = myFunc(x_sample) + np.random.normal(0, sigma, (n, 1))\n",
    "# plot the dataset \n",
    "plt.scatter(x_sample, y_sample)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Split dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "# define the size of the training set in percent. \n",
    "# Do not use too small or too high values as the remaining algorithms require to have a valid training and test dataset. \n",
    "training_perc = 70\n",
    "\n",
    "### Code: \n",
    "# calculate which index splits the data into training and validation set \n",
    "idx_split = int(np.floor(n*training_perc/100))\n",
    "if idx_split < 1: \n",
    "    print('Chosen training set is too small!\\n')\n",
    "if idx_split > (n-1): \n",
    "    print('Chosen training set is too big!\\n')\n",
    "\n",
    "# split the model into training and test sets\n",
    "x_training = x_sample[1:idx_split]\n",
    "x_test = x_sample[(idx_split+1):n]\n",
    "y_training = y_sample[1:idx_split]\n",
    "y_test = y_sample[(idx_split+1):n]\n",
    "# generate equally spaced x values to plot the identified model later\n",
    "x_plot = np.reshape(np.linspace(x_min, x_max, n), (n,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Polynomial Regression with Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_regression(ols_PolyDegree, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    ols_poly = sk_preprocessing.PolynomialFeatures(ols_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    ols_X_test = ols_poly.fit_transform(x_test)\n",
    "    ols_X_training = ols_poly.fit_transform(x_training)\n",
    "    ols_X_plot = ols_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    ols_regr = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    ols_regr.fit(ols_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    ols_y_pred_training = ols_regr.predict(ols_X_training)\n",
    "    ols_y_pred_test = ols_regr.predict(ols_X_test)\n",
    "    ols_y_plot = ols_regr.predict(ols_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', ols_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, ols_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, ols_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, ols_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return ols_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Polynomial Regression with L2 Regularization ( Ridge Regression )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code\n",
    "def ridge_regression(l2_PolyDegree, reg_lambda2, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    ridge_poly = sk_preprocessing.PolynomialFeatures(l2_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    ridge_X_test = ridge_poly.fit_transform(x_test)\n",
    "    ridge_X_training = ridge_poly.fit_transform(x_training)\n",
    "    ridge_X_plot = ridge_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    ridge_regr = sk_linear_model.Ridge(alpha=reg_lambda2, fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    ridge_regr.fit(ridge_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    ridge_y_pred_training = ridge_regr.predict(ridge_X_training)\n",
    "    ridge_y_pred_test = ridge_regr.predict(ridge_X_test)\n",
    "    ridge_y_plot = ridge_regr.predict(ridge_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', ridge_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, ridge_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, ridge_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, ridge_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "\n",
    "    return ridge_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Polynomial Regression with L1 regularization ( Lasso Regression )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code\n",
    "def lasso_regression(l1_PolyDegree, reg_lambda1, x_test, x_training, x_plot, y_test, y_training):\n",
    "    # Create a preprocessing object for the given polynomial degree configuration\n",
    "    lasso_poly = sk_preprocessing.PolynomialFeatures(l1_PolyDegree)\n",
    "    \n",
    "    # Create the design matrices for the test, the training and the plot points \n",
    "    lasso_X_test = lasso_poly.fit_transform(x_test)\n",
    "    lasso_X_training = lasso_poly.fit_transform(x_training)\n",
    "    lasso_X_plot = lasso_poly.fit_transform(x_plot)\n",
    "\n",
    "    # Create linear regression object\n",
    "    lasso_regr = sk_linear_model.Lasso(alpha=reg_lambda1, fit_intercept=False)\n",
    "    \n",
    "    # Train the model based on the training design matrix and the training output data\n",
    "    lasso_regr.fit(lasso_X_training, y_training)\n",
    "\n",
    "    # Predict the output data for the training, the test and the plot points\n",
    "    lasso_y_pred_training = lasso_regr.predict(lasso_X_training)\n",
    "    lasso_y_pred_test = lasso_regr.predict(lasso_X_test)\n",
    "    lasso_y_plot = lasso_regr.predict(lasso_X_plot)\n",
    "\n",
    "    # print the coefficients\n",
    "    print('Coefficients: \\n', lasso_regr.coef_)\n",
    "    # print the MSE for the training and the test dataset\n",
    "    print(\"Mean squared error on the training dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_training, lasso_y_pred_training))\n",
    "    print(\"Mean squared error on the test dataset: %.2f\"\n",
    "          % sk_metrics.mean_squared_error(y_test, lasso_y_pred_test))\n",
    "\n",
    "    # Plot the results of the test dataset and the identified model based on the plot points\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_plot, lasso_y_plot, color='blue', linewidth=3)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return lasso_y_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Comparison of Polynomial Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "ols_PolyDegree = 5\n",
    "### apply OLS regression\n",
    "ols_y_plot = ols_regression(ols_PolyDegree, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "l2_PolyDegree = 5\n",
    "reg_lambda2 = 50\n",
    "### apply ridge regression\n",
    "ridge_y_plot = ridge_regression(l2_PolyDegree, reg_lambda2, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters: \n",
    "l1_PolyDegree = 5\n",
    "reg_lambda1 = 0.1\n",
    "### apply lasso regression\n",
    "lasso_y_plot = lasso_regression(l1_PolyDegree, reg_lambda1, x_test, x_training, x_plot, y_test, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare data \n",
    "plt.scatter(x_test, y_test, color='black')\n",
    "plt.plot(x_plot, ols_y_plot, color='blue', linewidth=3, label='OLS')\n",
    "plt.plot(x_plot, ridge_y_plot, color='green', linewidth=3, label='Ridge')\n",
    "plt.plot(x_plot, lasso_y_plot, color='red', linewidth=3, label='Lasso')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powertrain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv \n",
    "data = np.genfromtxt('./data/Powertrain.csv', delimiter=',');\n",
    "# map input data to meaningful variables\n",
    "PowertrainForce_N = np.reshape(data[1:,0], (len(data)-1,1));\n",
    "LongAcceleration_mps2 = np.reshape(data[1:,1], (len(data)-1,1));\n",
    "LongVelocity_mps = np.reshape(data[1:,2], (len(data)-1,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PowertrainForce_N, LongAcceleration_mps2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(LongVelocity_mps, LongAcceleration_mps2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Velocity in mps');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Apply standard regression\n",
    " $$a_x = c_1F_{\\mathrm{PT}} + c_0$$\n",
    " $$a_x = \\frac{1}{m}F_{\\mathrm{PT}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_PolyDegree = 1; \n",
    "ols_poly = sk_preprocessing.PolynomialFeatures(ols_PolyDegree);\n",
    "ols_X_training = ols_poly.fit_transform(PowertrainForce_N);\n",
    "ols_regr = sk_linear_model.LinearRegression(fit_intercept=False);\n",
    "ols_regr.fit(ols_X_training, LongAcceleration_mps2);\n",
    "\n",
    "## visualize results\n",
    "PowertrainTestForce_N = np.reshape(np.linspace(-15000, 15000, 100), (100,1));\n",
    "ols_X_plot = ols_poly.fit_transform(PowertrainTestForce_N);\n",
    "ols_y_plot = ols_regr.predict(ols_X_plot);\n",
    "plt.scatter(PowertrainForce_N, LongAcceleration_mps2)\n",
    "plt.plot(PowertrainTestForce_N, ols_y_plot, color='black', linewidth=5)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');\n",
    "print('Coefficients: \\n', ols_regr.coef_)\n",
    "\n",
    "## calculate vehicle mass\n",
    "print('Vehicle mass: ', 1/ols_regr.coef_[0,1], 'kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PowertrainForce_N, ols_regr.predict(ols_X_training)-LongAcceleration_mps2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(LongVelocity_mps, ols_regr.predict(ols_X_training)-LongAcceleration_mps2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Velocity in mps');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant data point indices\n",
    "idx = np.where((PowertrainForce_N < 5000));\n",
    "PowertrainForce_N_red = PowertrainForce_N[idx];\n",
    "LongAcceleration_mps2_red = LongAcceleration_mps2[idx];\n",
    "LongVelocity_mps_red = LongVelocity_mps[idx];\n",
    "plt.scatter(PowertrainForce_N_red, LongAcceleration_mps2_red)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Apply regression\n",
    "\n",
    " $$a_x = c_1F_{\\mathrm{PT}} + c_0$$\n",
    " $$a_x = \\frac{1}{m}F_{\\mathrm{PT}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_PolyDegree = 1; \n",
    "ols_poly = sk_preprocessing.PolynomialFeatures(ols_PolyDegree);\n",
    "ols_X_training_red = ols_poly.fit_transform(np.reshape(PowertrainForce_N_red, (len(PowertrainForce_N_red),1)));\n",
    "ols_regr_out = sk_linear_model.LinearRegression(fit_intercept=False);\n",
    "ols_regr_out.fit(ols_X_training_red, LongAcceleration_mps2_red);\n",
    "\n",
    "## visualize results\n",
    "PowertrainTestForce_N = np.reshape(np.linspace(-15000, 15000, 100), (100,1));\n",
    "ols_X_plot = ols_poly.fit_transform(PowertrainTestForce_N);\n",
    "ols_y_plot = ols_regr_out.predict(ols_X_plot);\n",
    "plt.scatter(PowertrainForce_N, LongAcceleration_mps2)\n",
    "plt.plot(PowertrainTestForce_N, ols_y_plot, color='black', linewidth=5)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');\n",
    "print('Coefficients: \\n', ols_regr_out.coef_)\n",
    "\n",
    "## calculate vehicle mass\n",
    "print('Vehicle mass: ', 1/ols_regr_out.coef_[1], 'kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PowertrainForce_N_red, ols_regr_out.predict(ols_X_training_red)-LongAcceleration_mps2_red)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(LongVelocity_mps_red, ols_regr_out.predict(ols_X_training_red)-LongAcceleration_mps2_red)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Velocity in mps');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Apply regression with velocity as additional input\n",
    "\n",
    " $$a_x = c_1F_{\\mathrm{PT}} + c_2v^2$$\n",
    " $$a_x = \\frac{1}{m}\\left(F_{\\mathrm{PT}} + c_wv^2\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_PolyDegree = 1; \n",
    "ols_poly = sk_preprocessing.PolynomialFeatures(ols_PolyDegree);\n",
    "# form matrix with feature vectors \n",
    "InputMatrix = np.concatenate((np.reshape(PowertrainForce_N_red, (len(PowertrainForce_N_red),1)), np.reshape(LongVelocity_mps_red**2, (len(LongVelocity_mps_red),1))), axis=1);\n",
    "ols_regr_vel = sk_linear_model.LinearRegression(fit_intercept=False);\n",
    "ols_regr_vel.fit(InputMatrix, LongAcceleration_mps2_red);\n",
    "\n",
    "## visualize results\n",
    "PowertrainTestForce_N = np.reshape(np.linspace(-15000, 15000, 100), (100,1));\n",
    "PowertrainTestVelocity_N = np.zeros((100,1));\n",
    "ols_X_plot = np.concatenate((np.reshape(PowertrainTestForce_N, (len(PowertrainTestForce_N),1)), np.reshape(PowertrainTestVelocity_N**2, (len(PowertrainTestVelocity_N),1))), axis=1);\n",
    "ols_y_plot = ols_regr_vel.predict(ols_X_plot);\n",
    "plt.scatter(PowertrainForce_N, LongAcceleration_mps2)\n",
    "plt.plot(PowertrainTestForce_N, ols_y_plot, color='black', linewidth=5)\n",
    "plt.grid(True)\n",
    "print('Coefficients: \\n', ols_regr_vel.coef_)\n",
    "\n",
    "## calculate vehicle mass\n",
    "print('Vehicle mass: ', 1/ols_regr_vel.coef_[0], 'kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PowertrainForce_N_red, ols_regr_vel.predict(InputMatrix)-LongAcceleration_mps2_red)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Powertrain Force in N');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(LongVelocity_mps_red, ols_regr_vel.predict(InputMatrix)-LongAcceleration_mps2_red)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Velocity in mps');\n",
    "plt.ylabel('Long Acc in mps2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anscombes quartet can be found on Wikepedia \n",
    "# https://en.wikipedia.org/wiki/Anscombe%27s_quartet\n",
    "# Definition of the dataset:\n",
    "\n",
    "x1 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y1 = np.array([8.04, 6.95, 7.58,  8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "x2 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y2 = np.array([9.14, 8.14, 8.74,  8.77, 9.26, 8.10, 6.13, 3.10, 9.13,  7.26, 4.74])\n",
    "x3 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15,  6.42, 5.73])\n",
    "x4 = np.array([8.0,  8.0,  8.0,   8.0,  8.0,  8.0,  8.0,  19.0,  8.0,  8.0,  8.0])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Basic visual analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset to gain some insight on whats going on \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intersting thing about Anscombes Quartet is\n",
    "# that the datasets look very different from a graphical point of view \n",
    "# but are nearly similar from a basic statistics point of view. \n",
    "meanValues = np.array([np.mean(y1), np.mean(y2), np.mean(y3), np.mean(y4)])\n",
    "print(\"The mean values are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(meanValues[0], meanValues[1], meanValues[2], meanValues[3]))\n",
    "stdValues = np.array([np.std(y1), np.std(y2), np.std(y3), np.std(y4)])\n",
    "print(\"The standard deviations are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(stdValues[0], stdValues[1], stdValues[2], stdValues[3]))\n",
    "corrValues = np.array([np.corrcoef(x1, y1)[0,1], np.corrcoef(x2, y2)[0,1], np.corrcoef(x3, y3)[0,1], np.corrcoef(x4, y4)[0,1]])\n",
    "print(\"The correlation coefficients are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(corrValues[0], corrValues[1], corrValues[2], corrValues[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Short summary of what we've seen so far\n",
    " We have seen that the collection of four datasets called Anscombes Quartet looks very different from a visual point of view. However, applying basic statistic measures leads to the conclusion that the datasets are indeed very similar. This was exactly the intention for the formulation of this dataset. It was designed to emphasize the importance of visualization during statistical work with real world data. Furthermore, we can see that outliers can significantly reduce performance of basic statistic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Regression on that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features for all datasets using linear and quadratic functions \n",
    "polydeg1 = sk_preprocessing.PolynomialFeatures(1)\n",
    "polydeg2 = sk_preprocessing.PolynomialFeatures(2)\n",
    "\n",
    "# create design matrices for all datasets\n",
    "# reshape is necessary to create 2D array from 1D array \n",
    "ds1_pd1_x = polydeg1.fit_transform(x1.reshape(-1,1))\n",
    "ds2_pd1_x = polydeg1.fit_transform(x2.reshape(-1,1))\n",
    "ds3_pd1_x = polydeg1.fit_transform(x3.reshape(-1,1))\n",
    "ds4_pd1_x = polydeg1.fit_transform(x4.reshape(-1,1))\n",
    "ds1_pd2_x = polydeg2.fit_transform(x1.reshape(-1,1))\n",
    "ds2_pd2_x = polydeg2.fit_transform(x2.reshape(-1,1))\n",
    "ds3_pd2_x = polydeg2.fit_transform(x3.reshape(-1,1))\n",
    "ds4_pd2_x = polydeg2.fit_transform(x4.reshape(-1,1))\n",
    "\n",
    "# create regression model objects \n",
    "ds1_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds2_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds3_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds4_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds1_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds2_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds3_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds4_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Train the models based on the training design matrix and the training output data\n",
    "ds1_pd1_reg.fit(ds1_pd1_x, y1)\n",
    "ds2_pd1_reg.fit(ds2_pd1_x, y2)\n",
    "ds3_pd1_reg.fit(ds3_pd1_x, y3)\n",
    "ds4_pd1_reg.fit(ds4_pd1_x, y4)\n",
    "ds1_pd2_reg.fit(ds1_pd2_x, y1)\n",
    "ds2_pd2_reg.fit(ds2_pd2_x, y2)\n",
    "ds3_pd2_reg.fit(ds3_pd2_x, y3)\n",
    "ds4_pd2_reg.fit(ds4_pd2_x, y4)\n",
    "\n",
    "# Predict the output data for the evaluation points \n",
    "x_eval = np.linspace(0, 20, num=20)\n",
    "ds1_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds2_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds3_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds4_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds1_pd2_x_eval= polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds2_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds3_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds4_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "\n",
    "ds1_pd1_y_eval = ds1_pd1_reg.predict(ds1_pd1_x_eval)\n",
    "ds2_pd1_y_eval = ds2_pd1_reg.predict(ds2_pd1_x_eval)\n",
    "ds3_pd1_y_eval = ds3_pd1_reg.predict(ds3_pd1_x_eval)\n",
    "ds4_pd1_y_eval = ds4_pd1_reg.predict(ds4_pd1_x_eval)\n",
    "ds1_pd2_y_eval = ds1_pd2_reg.predict(ds1_pd2_x_eval)\n",
    "ds2_pd2_y_eval = ds2_pd2_reg.predict(ds2_pd2_x_eval)\n",
    "ds3_pd2_y_eval = ds3_pd2_reg.predict(ds3_pd2_x_eval)\n",
    "ds4_pd2_y_eval = ds4_pd2_reg.predict(ds4_pd2_x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize linear fits \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "plot(x_eval, ds1_pd1_y_eval,'r')\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "plot(x_eval, ds2_pd1_y_eval,'r')\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "plot(x_eval, ds3_pd1_y_eval,'r')\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()\n",
    "plot(x_eval, ds4_pd1_y_eval,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Comments on the results\n",
    "\n",
    " We have pointed out before, that the basic statistics of all four datasets are nearly equal. Why this might not be a problem in some applications, applying a purely linear model to the dataset shows that this can lead to severe problems. From a visual perspective, everyone would agree that some of the models do not represent the data at all. However, they are optimal in a sense that they minimize the loss function. The quartet therefore emphasizes the importance of data visualization especially if the amount and type of outliers and general behavior of the data is unknown! Furthermore, these datasets can be viewed as warning examples when applying regression methods online in fully autonomos machines. The algorithms have to be capable of dealing with such situations if they may occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize quadratic fits \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "plot(x_eval, ds1_pd2_y_eval,'r')\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "plot(x_eval, ds2_pd2_y_eval,'r')\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "plot(x_eval, ds3_pd2_y_eval,'r')\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()\n",
    "plot(x_eval, ds4_pd2_y_eval,'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
